---
output:
  html_document: default
  pdf_document: default
---

Resampling Methods
================

We are going to use the Auto data to illustrate the results of various re-sampling methods, so lets load it from the `ISLR` package and explore. 

```{r}
library(ISLR)
data(Auto)
```

Exploring the available documentation as well as the structure of the data is usually a good place to start.

```{r, eval=FALSE}
?Auto
```
```{r, tidy=TRUE}
str(Auto[,-9])
```

A plot is always a nice place to start with a new data set.
```{r, tidy=TRUE}
plot(mpg ~ horsepower, data = Auto)
```



## The Leave-One-Out Cross-Validation (LCOOV) method.

First, lets run a `glm` model on the `Auto` data set.

```{r}
glm_auto <- glm(mpg ~ horsepower, data = Auto)
```

Next, load the `boot` package and check out the documentation for the Cross-validation for Generalized Linear Models function, or `cv.glm`.

```{r}
library(boot)
```
```{r, eval=FALSE}
?cv.glm
```

Then, apply `cv.glm` function to the `Auto` data set, using `glm_auto` model,
returning the delta parameter.

```{r}
cv.glm(Auto, glm_auto)$delta 
```

We can speed up the results by writing a function to use the formula displayed in section 5.2 (pg. 180) and then pass the `glm_auto` model to it.

```{r}
loocv <- function(x){
          h <- lm.influence(x)$h
        mean((residuals(x)/(1-h))^2)
}
```

Is our new function faster? We can use the `system.time` function to compare both methods.

```{r}
system.time(
cv.glm(Auto, glm_auto)$delta 
)

system.time(
loocv(glm_auto)
)
```

\newpage

Next, lets use a `for` loop to efficiently create 5 new polynomial versions of the previous model, regressing `horsepower` against `mpg` and see if the results improve as polynomial order increases.

```{r}
cv.error <- rep(0, 5)
degree <- 1:5

for(d in degree){
  glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
  cv.error[d] <- loocv(glm.fit)
}

plot(degree, cv.error, type = "b", col = "blue", pch = 16,
     main = "LOOCV", xlab = "Degree of Polynomial")
```

\newpage

## The10-fold Cross-Validation

First, initially the cv.error10 vector and ten run the loop.
```{r}
cv.error10 <- rep(0, 5)

for(d in degree){
  glm.fit <- glm(mpg ~ poly(horsepower, d), data = Auto)
  cv.error10[d] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}

plot(degree, cv.error, type = "b", col = "blue", pch = 16, 
     main = "10 Fold CV", xlab = "Degree of Polynomial")
lines(degree, cv.error10, type = "b", col = "red", pch = 16)
```

## Bootstrap

Suppose that we wish to invest a ﬁxed sum of money in two ﬁnancial assets that yield returns of X and Y, where X and Y are random quantities. We will invest a fraction of our money in X, and will invest the remaining 1 - \alpha in Y. We wish to choose \alpha to minimize the total risk, or variance, of our investment. In other words, we want to minimize $Var(\alpha X + (1-\alpha)Y). One can show that the value that minimizes the risk is given by

$$\alpha = \frac{\sigma^2_Y - \sigma_{XY}}{\sigma^2_X + \sigma^2_Y - 2\sigma_{XY}}$$

where $\sigma^2_X = Var(X)$, $\sigma^2_Y = Var(Y)$, and $\sigma_{XY} = Cov(X,Y)$.

However, the values of $\sigma^2_X$, $\sigma^2_Y$, and  $\sigma_{XY}$ are unkown.
We can compute estimates for these quantities, $\hat\sigma^2_X$, $\hat\sigma^2_Y$, and  $\hat\sigma_{XY}$, using a data set that contains measurments for $X$ and $Y$.

We can then estimate the value of \alpha that minimizes the variance of our investment using:

$$\hat\alpha = \frac{\hat\sigma^2_Y - \hat\sigma_{XY}}{\hat\sigma^2_X + \hat\sigma^2_Y - 2\hat\sigma_{XY}}$$

Load the `Portfolio` data set from the `ISLR` package, containing 100 returns for two assets, X and Y.

```{r}
data("Portfolio")
```

```{r}
plot(Y ~ X, data = Portfolio, col = "darkgreen", type = "p", pch = 16)
```

```{r}
alpha <- function(x, y) {
        
          var_x <- var(x)
          var_y <- var(y)
          cov_xy <- cov(x, y)
          
(var_y - cov_xy)/(var_x + var_y - 2 *cov_xy)
          
}

alpha(Portfolio$X, Portfolio$Y)
```

So what is the standard error of alpha?
First, lets make a wrapper function

```{r}
alpha2 <- function(data, index){
                   with(data[index, ], alpha(X, Y))
}

alpha2(Portfolio, 1:100)

set.seed(1)
alpha2(Portfolio, sample(1:100, 100, replace = TRUE))

boot.out <- boot(Portfolio, alpha2, R = 1000)

boot.out$t0
```

Finally, plot the bootstrap model object.
```{r}
plot(boot.out)
```

## 5.R.R1

Download the file 5.R.RData and load it into R using `load("5.R.RData")`. Consider the linear regression model of y on X1 and X2. What is the standard error for $\beta_1$?

```{r}
load(path.expand("~/R/Statistical-Learning/data/5.R.RData"))

model_51 <- lm(y ~ X1 + X2, data = Xy)

summary(model_51)
```

## 5.R.R2

Next, plot the data using `matplot(Xy, type="l")`. Which of the following do you think is most likely given what you see?

```{r}
matplot(Xy, type="l")
```

5.R.R3

Now, use the (standard) bootstrap to estimate s.e.($\hat\beta_{1}$). 
To within 10%, what do you get?

```{r}
beta_hat <- function(data, index, formula){
        
               model <- lm(formula, data = data[index, ])
        
        summary(model)$coefficients[,1][[2]]
}

boot(data = Xy, statistic = beta_hat, R = 100000, formula = y~X1+X2, parallel = "snow", ncpus = 4)
```

## 5.R.R4


Finally, use the block bootstrap to estimate s.e.($\hat\beta_1$). Use blocks of 100 contiguous observations, and resample ten whole blocks with replacement then paste them together to construct each bootstrap time series. For example, one of your bootstrap resamples could be:

```{r}
new.rows = c(101:200, 401:500, 101:200, 901:1000, 301:400, 1:100, 1:100, 801:900, 201:300, 701:800)

new.Xy = Xy[new.rows, ]
```

